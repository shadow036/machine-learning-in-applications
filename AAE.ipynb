{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Model\n",
    "from keras import layers, losses\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from keras.utils import plot_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "try:\n",
    "    # enable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type == 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "DATA_DIR_SLOW = \"/Users/kedi/Desktop/polito lectures and notes/ML in App/project/KukaVelocityDataset/KukaSlow.npy\"\n",
    "DATA_DIR_NORMAL = \"/Users/kedi/Desktop/polito lectures and notes/ML in App/project/KukaVelocityDataset/KukaNormal.npy\"\n",
    "DATA_DIR_C_NAMES = \"/Users/kedi/Desktop/polito lectures and notes/ML in App/project/KukaVelocityDataset/KukaColumnNames.npy\"\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use(\"Solarize_Light2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "data_normal = np.load(DATA_DIR_NORMAL)\n",
    "data_slow = np.load(DATA_DIR_SLOW)\n",
    "column_names = np.load(DATA_DIR_C_NAMES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action' 'machine_nameKuka Robot_apparent_power'\n",
      " 'machine_nameKuka Robot_current' 'machine_nameKuka Robot_frequency'\n",
      " 'machine_nameKuka Robot_phase_angle' 'machine_nameKuka Robot_power'\n",
      " 'machine_nameKuka Robot_power_factor'\n",
      " 'machine_nameKuka Robot_reactive_power' 'machine_nameKuka Robot_voltage'\n",
      " 'sensor_id1_AccX' 'sensor_id1_AccY' 'sensor_id1_AccZ' 'sensor_id1_GyroX'\n",
      " 'sensor_id1_GyroY' 'sensor_id1_GyroZ' 'sensor_id1_q1' 'sensor_id1_q2'\n",
      " 'sensor_id1_q3' 'sensor_id1_q4' 'sensor_id1_temp' 'sensor_id2_AccX'\n",
      " 'sensor_id2_AccY' 'sensor_id2_AccZ' 'sensor_id2_GyroX' 'sensor_id2_GyroY'\n",
      " 'sensor_id2_GyroZ' 'sensor_id2_q1' 'sensor_id2_q2' 'sensor_id2_q3'\n",
      " 'sensor_id2_q4' 'sensor_id2_temp' 'sensor_id3_AccX' 'sensor_id3_AccY'\n",
      " 'sensor_id3_AccZ' 'sensor_id3_GyroX' 'sensor_id3_GyroY'\n",
      " 'sensor_id3_GyroZ' 'sensor_id3_q1' 'sensor_id3_q2' 'sensor_id3_q3'\n",
      " 'sensor_id3_q4' 'sensor_id3_temp' 'sensor_id4_AccX' 'sensor_id4_AccY'\n",
      " 'sensor_id4_AccZ' 'sensor_id4_GyroX' 'sensor_id4_GyroY'\n",
      " 'sensor_id4_GyroZ' 'sensor_id4_q1' 'sensor_id4_q2' 'sensor_id4_q3'\n",
      " 'sensor_id4_q4' 'sensor_id4_temp' 'sensor_id5_AccX' 'sensor_id5_AccY'\n",
      " 'sensor_id5_AccZ' 'sensor_id5_GyroX' 'sensor_id5_GyroY'\n",
      " 'sensor_id5_GyroZ' 'sensor_id5_q1' 'sensor_id5_q2' 'sensor_id5_q3'\n",
      " 'sensor_id5_q4' 'sensor_id5_temp' 'sensor_id6_AccX' 'sensor_id6_AccY'\n",
      " 'sensor_id6_AccZ' 'sensor_id6_GyroX' 'sensor_id6_GyroY'\n",
      " 'sensor_id6_GyroZ' 'sensor_id6_q1' 'sensor_id6_q2' 'sensor_id6_q3'\n",
      " 'sensor_id6_q4' 'sensor_id6_temp' 'sensor_id7_AccX' 'sensor_id7_AccY'\n",
      " 'sensor_id7_AccZ' 'sensor_id7_GyroX' 'sensor_id7_GyroY'\n",
      " 'sensor_id7_GyroZ' 'sensor_id7_q1' 'sensor_id7_q2' 'sensor_id7_q3'\n",
      " 'sensor_id7_q4' 'sensor_id7_temp' 'anomaly']\n"
     ]
    }
   ],
   "source": [
    "print(column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Normalization train on train set transform both\n",
    "scaler = preprocessing.StandardScaler().fit(data_normal)\n",
    "data_train_scaled = scaler.transform(data_normal)\n",
    "data_test_scaled = scaler.transform(data_slow[:,:-1]) # drop the last colum because it is just 1 that represents anomaly \"true\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (23379, 86) \n",
      " test data shape: (4153, 86) \n"
     ]
    }
   ],
   "source": [
    "# take part of data to gather results faster\n",
    "percentage = 0.10\n",
    "data_train = data_train_scaled[0:int(percentage*data_train_scaled.shape[0]),:]\n",
    "data_test = data_test_scaled[0:int(percentage*data_test_scaled.shape[0]),:]\n",
    "print(f\"train data shape: {data_train.shape} \\n test data shape: {data_test.shape} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderGenerator(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_dims,\n",
    "        input_size,\n",
    "        input_channels,\n",
    "        step_channels=16,\n",
    "        nonlinearity=tf.keras.layers.LeakyReLU(0.2),\n",
    "    ):\n",
    "        super(AdversarialAutoencoderGenerator, self).__init__()\n",
    "        encoder = [\n",
    "            tf.keras.layers.Conv1D(step_channels, 5, strides=2, padding='same', activation=nonlinearity)\n",
    "        ]\n",
    "        size = input_size // 2\n",
    "        channels = step_channels\n",
    "        while size > 1:\n",
    "            encoder.append(\n",
    "                tf.keras.layers.Conv1D(channels * 4, 5, strides=4, padding='same')\n",
    "            )\n",
    "            encoder.append(tf.keras.layers.BatchNormalization())\n",
    "            encoder.append(nonlinearity)\n",
    "            channels *= 4\n",
    "            size = size // 4\n",
    "        self.encoder = tf.keras.Sequential(encoder)\n",
    "        self.encoder_fc = tf.keras.layers.Dense(encoding_dims)\n",
    "        self.decoder_fc = tf.keras.layers.Dense(step_channels * size)\n",
    "        decoder = []\n",
    "        size = 1\n",
    "        channels = step_channels\n",
    "        while size < input_size // 2:\n",
    "            decoder.append(\n",
    "                tf.keras.layers.Conv1DTranspose(channels * 4, 5, strides=4, padding='same')\n",
    "            )\n",
    "            decoder.append(tf.keras.layers.BatchNormalization())\n",
    "            decoder.append(nonlinearity)\n",
    "            channels *= 4\n",
    "            size *= 4\n",
    "        decoder.append(tf.keras.layers.Conv1DTranspose(input_channels, 5, strides=2, padding='same'))\n",
    "        self.decoder = tf.keras.Sequential(decoder)\n",
    "\n",
    "    def sample(self, noise):\n",
    "        noise = self.decoder_fc(noise)\n",
    "        noise = tf.reshape(noise, (-1, 1, noise.shape[1]))\n",
    "        return self.decoder(noise)\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.training:\n",
    "            encoding = self.encoder(x)\n",
    "            encoding = tf.reshape(encoding, (-1, tf.reduce_prod(encoding.shape[1:])))\n",
    "            encoding = self.encoder_fc(encoding)\n",
    "            return self.sample(encoding), encoding\n",
    "        else:\n",
    "            return self.sample(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, input_dims, non_linearity=tf.keras.layers.LeakyReLU(0.2)):\n",
    "        super(AdversarialAutoencoderDiscriminator, self).__init__()\n",
    "        model = [tf.keras.layers.Dense(input_dims // 2, activation=non_linearity)]\n",
    "        size = input_dims // 2\n",
    "        while size > 16:\n",
    "            model.append(tf.keras.layers.Dense(size // 2))\n",
    "            model.append(tf.keras.layers.BatchNormalization())\n",
    "            model.append(non_linearity)\n",
    "            size = size // 2\n",
    "        model.append(tf.keras.layers.Dense(1))\n",
    "        self.model = tf.keras.Sequential(model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderGeneratorLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(AdversarialAutoencoderGeneratorLoss, self).__init__()\n",
    "\n",
    "    def call(self, real_inputs, gen_inputs, dgz):\n",
    "        mse_loss = tf.keras.losses.MeanSquaredError()(gen_inputs, real_inputs)\n",
    "        bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(dgz, tf.ones_like(dgz))\n",
    "        loss = 0.999 * mse_loss + 0.001 * bce_loss\n",
    "        return loss\n",
    "\n",
    "    def train_ops(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        optimizer_generator,\n",
    "        real_inputs,\n",
    "        device,\n",
    "        batch_size,\n",
    "        labels=None,\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            recon, encodings = generator(real_inputs)\n",
    "            dgz = discriminator(encodings)\n",
    "            loss = self.call(real_inputs, recon, dgz)\n",
    "        gradients = tape.gradient(loss, generator.trainable_variables)\n",
    "        optimizer_generator.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "        return loss.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class AdversarialAutoencoderDiscriminatorLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(AdversarialAutoencoderDiscriminatorLoss, self).__init__()\n",
    "\n",
    "    def call(self, dx, dgz):\n",
    "        bce_loss_real = tf.keras.losses.BinaryCrossentropy(from_logits=True)(dx, tf.ones_like(dx))\n",
    "        bce_loss_fake = tf.keras.losses.BinaryCrossentropy(from_logits=True)(dgz, tf.zeros_like(dgz))\n",
    "        loss = 0.5 * bce_loss_real + 0.5 * bce_loss_fake\n",
    "        return loss\n",
    "\n",
    "    def train_ops(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        optimizer_discriminator,\n",
    "        real_inputs,\n",
    "        device,\n",
    "        batch_size,\n",
    "        labels=None,\n",
    "    ):\n",
    "        _, encodings = generator(real_inputs)\n",
    "        noise = tf.random.normal((real_inputs.shape[0], generator.encoding_dims), device=device)\n",
    "        with tf.GradientTape() as tape:\n",
    "            dx = discriminator(noise)\n",
    "            dgz = discriminator(encodings)\n",
    "            loss = self.call(dx, dgz)\n",
    "        gradients = tape.gradient(loss, discriminator.trainable_variables)\n",
    "        optimizer_discriminator.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
    "        return loss.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "losses = [\n",
    "    AdversarialAutoencoderGeneratorLoss(),\n",
    "    AdversarialAutoencoderDiscriminatorLoss(),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "network = {\n",
    "    \"generator\": {\n",
    "        \"name\": AdversarialAutoencoderGenerator,\n",
    "        \"args\": {\"encoding_dims\": 128, \"input_size\": 86, \"input_channels\": 1},\n",
    "        \"optimizer\": {\"name\": tf.keras.optimizers.Adam, \"args\": {\"learning_rate\": 0.0002, \"beta_1\": 0.5, \"beta_2\": 0.999}},\n",
    "    },\n",
    "    \"discriminator\": {\n",
    "        \"name\": AdversarialAutoencoderDiscriminator,\n",
    "        \"args\": {\"input_dims\": 128,},\n",
    "        \"optimizer\": {\"name\": tf.keras.optimizers.Adam, \"args\": {\"learning_rate\": 0.0002, \"beta_1\": 0.5, \"beta_2\": 0.999}},\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self, network, losses, sample_size, epochs, device):\n",
    "        self.generator = network[\"generator\"][\"name\"](**network[\"generator\"][\"args\"])\n",
    "        self.discriminator = network[\"discriminator\"][\"name\"](**network[\"discriminator\"][\"args\"])\n",
    "        self.generator_optimizer = network[\"generator\"][\"optimizer\"][\"name\"](**network[\"generator\"][\"optimizer\"][\"args\"])\n",
    "        self.discriminator_optimizer = network[\"discriminator\"][\"optimizer\"][\"name\"](**network[\"discriminator\"][\"optimizer\"][\"args\"])\n",
    "        self.losses = losses\n",
    "        self.sample_size = sample_size\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, train_data):\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch in train_data:\n",
    "                real_inputs = batch  # Assuming each batch is a tensor of shape (batch_size, input_dims)\n",
    "\n",
    "                with tf.device(self.device):\n",
    "                    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                        recon, encodings = self.generator(real_inputs)\n",
    "                        dx = self.discriminator(real_inputs)\n",
    "                        dgz = self.discriminator(encodings)\n",
    "\n",
    "                        generator_loss = self.losses.generator_loss(real_inputs, recon, dgz)\n",
    "                        discriminator_loss = self.losses.discriminator_loss(dx, dgz)\n",
    "\n",
    "                    generator_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
    "                    discriminator_gradients = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                    self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "                    self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "                    total_loss += generator_loss + discriminator_loss\n",
    "                    num_batches += 1\n",
    "\n",
    "            average_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {average_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "trainer = Trainer(network, losses, sample_size=64, epochs=128, device=\"/GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"adversarial_autoencoder_generator_5\" (type AdversarialAutoencoderGenerator).\n\n'AdversarialAutoencoderGenerator' object has no attribute 'training'\n\nCall arguments received by layer \"adversarial_autoencoder_generator_5\" (type AdversarialAutoencoderGenerator):\n  • x=tf.Tensor(shape=(86,), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[92], line 22\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_data)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m gen_tape, tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m disc_tape:\n\u001B[0;32m---> 22\u001B[0m         recon, encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreal_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m         dx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator(real_inputs)\n\u001B[1;32m     24\u001B[0m         dgz \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator(encodings)\n",
      "File \u001B[0;32m~/miniforge3/envs/tensor_gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[0;32mIn[86], line 47\u001B[0m, in \u001B[0;36mAdversarialAutoencoderGenerator.call\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m:\n\u001B[1;32m     48\u001B[0m         encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[1;32m     49\u001B[0m         encoding \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreshape(encoding, (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, tf\u001B[38;5;241m.\u001B[39mreduce_prod(encoding\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m:])))\n",
      "\u001B[0;31mAttributeError\u001B[0m: Exception encountered when calling layer \"adversarial_autoencoder_generator_5\" (type AdversarialAutoencoderGenerator).\n\n'AdversarialAutoencoderGenerator' object has no attribute 'training'\n\nCall arguments received by layer \"adversarial_autoencoder_generator_5\" (type AdversarialAutoencoderGenerator):\n  • x=tf.Tensor(shape=(86,), dtype=float32)"
     ]
    }
   ],
   "source": [
    "trainer.train(data_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}